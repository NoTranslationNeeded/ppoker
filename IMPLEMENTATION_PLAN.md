# Texas Hold'em AI í•™ìŠµ í”„ë¡œì íŠ¸ - êµ¬í˜„ ê³„íš

## ğŸ¯ í”„ë¡œì íŠ¸ ëª©í‘œ

**ìµœì¢… ëª©í‘œ**: ì»¤ìŠ¤í…€ POKERENGINEì„ ì‚¬ìš©í•˜ì—¬ í—¤ì¦ˆì—… ë…¸ë¦¬ë°‹ í…ì‚¬ìŠ¤ í™€ë¤ AIë¥¼ ê°•í™”í•™ìŠµ(Reinforcement Learning)ìœ¼ë¡œ í›ˆë ¨

**í•µì‹¬ ëª©í‘œ**:
- GTO(Game Theory Optimal) ì „ëµì— ê·¼ì ‘í•˜ëŠ” AI ê°œë°œ
- Self-playë¥¼ í†µí•œ ìê°€ í•™ìŠµ
- ë‹¤ì–‘í•œ ìƒí™©ì—ì„œ ì ì‘ ê°€ëŠ¥í•œ ì „ëµ í•™ìŠµ
- ì¸ê°„ í”Œë ˆì´ì–´ì™€ ëŒ€ê²° ê°€ëŠ¥í•œ ìˆ˜ì¤€

**ë²”ìœ„**:
- 2ì¸ í—¤ì¦ˆì—… ë…¸ë¦¬ë°‹ í™€ë¤ì— ì§‘ì¤‘
- ë©€í‹°í”Œë ˆì´ì–´ëŠ” 1ë‹¨ê³„ ì™„ë£Œ í›„ ê³ ë ¤

---

## ğŸ› ï¸ ê¸°ìˆ  ìŠ¤íƒ

### í•µì‹¬ í”„ë ˆì„ì›Œí¬
- **Python 3.10+** - ê°œë°œ ì–¸ì–´
- **Ray/RLlib 2.x** - ê°•í™”í•™ìŠµ í”„ë ˆì„ì›Œí¬
- **PyTorch** - ì‹ ê²½ë§ ë°±ì—”ë“œ
- **Gymnasium** - í™˜ê²½ ì¸í„°í˜ì´ìŠ¤
- **POKERENGINE** - ì»¤ìŠ¤í…€ í¬ì»¤ ì—”ì§„ (TDA ê·œì¹™ ì¤€ìˆ˜)

### ëª¨ë‹ˆí„°ë§ & ë¶„ì„
- **TensorBoard** - í•™ìŠµ ë©”íŠ¸ë¦­ ì‹œê°í™”
- **MLflow** - ì‹¤í—˜ ê´€ë¦¬ (ì„ íƒ)

### ê°œë°œ ë„êµ¬
- **Git** - ë²„ì „ ê´€ë¦¬
- **pytest** - í…ŒìŠ¤íŠ¸

---

## ğŸ® Gymnasium í™˜ê²½ ì¸í„°í˜ì´ìŠ¤ ì„¤ê³„

### ê¸°ìˆ  ìŠ¤íƒ
- **Gymnasium 0.29+** - í‘œì¤€ RL í™˜ê²½ ì¸í„°í˜ì´ìŠ¤
- **NumPy 1.24+** - íš¨ìœ¨ì ì¸ ë°°ì—´ ì—°ì‚° ë° ë°ì´í„° ì²˜ë¦¬
- **POKERENGINE** - ì»¤ìŠ¤í…€ TDA ì¤€ìˆ˜ ê²Œì„ ë¡œì§

### í™˜ê²½ í´ë˜ìŠ¤ êµ¬ì¡°

```python
from ray.rllib.env.multi_agent_env import MultiAgentEnv
from gymnasium import spaces
import numpy as np
from poker_engine import PokerGame, Action, ActionType

class PokerMultiAgentEnv(MultiAgentEnv):
    """
    2-player Heads-up No-Limit Texas Hold'em Multi-Agent Environment
    
    í•µì‹¬ ì„¤ê³„:
    - MultiAgentEnv ì‚¬ìš© (gym.Env ì•„ë‹˜!)
    - Self-Playì™€ ë¶„ì‚° í•™ìŠµ ì§€ì›
    - ê° í”Œë ˆì´ì–´ê°€ ë…ë¦½ì ì¸ ì •ì±… ì‚¬ìš© ê°€ëŠ¥
    
    Compatible with:
    - RLlib multi-agent training
    - Self-play (Phase 3)
    - League training (Phase 2)
    - NumPy-based observations
    """
    
    metadata = {'render_modes': ['human'], 'render_fps': 1}
    
    def __init__(self, config=None):
        super().__init__()
        
        config = config or {}
        
        # â­ BB ê³ ì • (ì ˆëŒ€ê°’ì€ ë¬´ì˜ë¯¸, BB ë‹¨ìœ„ë§Œ ì¤‘ìš”)
        self.small_blind = 50.0
        self.big_blind = 100.0
        
        # â­ ìŠ¤íƒ ê¹Šì´ ë¶„í¬ (BB ë‹¨ìœ„)
        # ì¤‘ìš”ë„ ê¸°ë°˜ ìƒ˜í”Œë§
        self.stack_distribution = {
            'standard': (80, 120, 0.40),   # 100BB Â±20, 40% í™•ë¥ 
            'middle': (20, 50, 0.30),      # 20-50BB, 30% í™•ë¥ 
            'short': (5, 20, 0.20),        # 5-20BB, 20% í™•ë¥ 
            'deep': (150, 250, 0.10)       # 150-250BB, 10% í™•ë¥ 
        }
        
        # Observation: NumPy array (338 floats)
        # 119: ì¹´ë“œ one-hot
        # 20: ê²Œì„ ìƒíƒœ + Legal Actions Mask
        # 20: ìŠ¤íŠ¸ë¦¬íŠ¸ë³„ ìš”ì•½ (Context) â† NEW!
        # 179: ìŠ¤íŠ¸ë¦¬íŠ¸ë³„ ì•¡ì…˜ íˆìŠ¤í† ë¦¬ (4Ã—4Ã—11)
        self.observation_space = spaces.Box(
            low=0.0,
            high=2.5,
            shape=(338,),
            dtype=np.float32
        )
        
        # Action: Discrete(8)
        self.action_space = spaces.Discrete(8)
        
        # Multi-agent ì„¤ì •
        self._agent_ids = {"player_0", "player_1"}
    
    def _sample_stack_depth(self) -> float:
        """ì¤‘ìš”ë„ ê¸°ë°˜ ìŠ¤íƒ ê¹Šì´ ìƒ˜í”Œë§"""
        categories = ['standard', 'middle', 'short', 'deep']
        probs = [0.40, 0.30, 0.20, 0.10]
        
        # ì¹´í…Œê³ ë¦¬ ì„ íƒ
        category = np.random.choice(categories, p=probs)
        min_bb, max_bb, _ = self.stack_distribution[category]
        
        # ë²”ìœ„ ë‚´ ëœë¤
        stack_bb = np.random.uniform(min_bb, max_bb)
        
        # ì¹© ìˆ˜ë¡œ ë³€í™˜
        return stack_bb * self.big_blind
    
    def reset(self, *, seed=None, options=None):
        """
        ë§¤ í•¸ë“œë§ˆë‹¤ í˜¸ì¶œ!
        ìŠ¤íƒê³¼ ë²„íŠ¼ì„ ëœë¤í™”í•˜ì—¬ ë‹¤ì–‘í•œ ìƒí™© í•™ìŠµ
        """
        if seed is not None:
            np.random.seed(seed)
        
        # â­ ìŠ¤íƒ ëœë¤ ìƒ˜í”Œë§ (ì¤‘ìš”ë„ ê¸°ë°˜)
        self.chips = [
            self._sample_stack_depth(),
            self._sample_stack_depth()
        ]
        
        # â­ ë²„íŠ¼ ìœ„ì¹˜ ëœë¤í™” (50:50)
        self.button = np.random.randint(0, 2)
        
        # í•¸ë“œ ì‹œì‘
        self.game = PokerGame(
            small_blind=self.small_blind,
            big_blind=self.big_blind
        )
        self.game.start_hand(
            players_info=[(0, self.chips[0]), (1, self.chips[1])],
            button=self.button  # â­ ëœë¤ ë²„íŠ¼!
        )
        
        # ì•¡ì…˜ íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
        self.action_history = {
            'preflop': [],
            'flop': [],
            'turn': [],
            'river': []
        }
        
        # í˜„ì¬ í”Œë ˆì´ì–´ ê´€ì°° ë°˜í™˜
        current_player = self.game.get_current_player()
        obs_dict = {f"player_{current_player}": self._get_observation(current_player)}
        info_dict = {
            f"player_{current_player}": {
                'legal_actions': self._get_legal_actions_mask(current_player),
                'button': self.button,
                'stacks': self.chips
            }
        }
        
        return obs_dict, info_dict
```

---

### 1. ê´€ì°° ê³µê°„ (Observation Space)

**íƒ€ì…**: `spaces.Box(shape=(150,), dtype=np.float32)`

**NumPy ë°°ì—´ êµ¬ì¡°** (150 floats):

#### ì¹´ë“œ One-Hot ì¸ì½”ë”© (0-118)
**ë¹„ê³µê°œ ì¹´ë“œ í‘œí˜„**: All-zero vector (One-hotì˜ ìì—°ìŠ¤ëŸ¬ìš´ "ì—†ìŒ" í‘œí˜„)
- Preflop ì•¡ì…˜ì´ í•¸ë“œ ë²”ìœ„ ê²°ì •

**River ë‹¨ê³„ ì „ì²´ ê´€ì°° ì˜ˆì‹œ**:
```
River ë‹¨ê³„ì—ì„œ:
- obs[150:190]: Preflop ì•¡ì…˜ + ëˆ„ê°€ + ì •í™•í•œ ë² íŒ… ë¹„ìœ¨ âœ…
- obs[190:230]: Flop ì•¡ì…˜ + ëˆ„ê°€ + ì •í™•í•œ ë² íŒ… ë¹„ìœ¨ âœ…
- obs[230:270]: Turn ì•¡ì…˜ + ëˆ„ê°€ + ì •í™•í•œ ë² íŒ… ë¹„ìœ¨ âœ…
- obs[270:310]: River í˜„ì¬ ì•¡ì…˜ + ëˆ„ê°€ + ì •í™•í•œ ë² íŒ… ë¹„ìœ¨

â†’ AIê°€ "ìŠ¤í† ë¦¬", "ê³µê²©/ë°©ì–´", "Bet Sizing Tell"ì„ ëª¨ë‘ ì´í•´!
```

#### í™•ì¥ ì˜ì—­



**êµ¬í˜„ ì˜ˆì‹œ** (NumPy One-Hot):
```python
def _encode_card_onehot(self, card) -> np.ndarray:
    """
    ì¹´ë“œë¥¼ 17ì°¨ì› one-hot ë²¡í„°ë¡œ ì¸ì½”ë”©
    
    [Rank: 13ì°¨ì›] + [Suit: 4ì°¨ì›] = 17ì°¨ì›
    
    ì˜ˆ: â™ 3 = [0,0,1,0,0,0,0,0,0,0,0,0,0, 1,0,0,0]
             â””â”€â”€â”€â”€â”€â”€â”€ Rank 3 â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€ Spade â”€â”˜
    """
    encoding = np.zeros(17, dtype=np.float32)
    
    # Rank one-hot (0-12): 2=0, 3=1, ..., A=12
    rank_idx = Card.RANKS.index(card.rank)
    encoding[rank_idx] = 1.0
    
    # Suit one-hot (13-16): S=13, H=14, D=15, C=16
    suit_idx = Card.SUITS.index(card.suit)
    encoding[13 + suit_idx] = 1.0
    
    return encoding

def _get_observation(self, player_id: int) -> np.ndarray:
    obs = np.zeros(150, dtype=np.float32)
    
    # === ì¹´ë“œ One-Hot ì¸ì½”ë”© (0-118) ===
    
    # 1. ë‚´ í™€ ì¹´ë“œ (0-33)
    for i, card in enumerate(self.game.players[player_id].hand[:2]):
        obs[i*17:(i+1)*17] = self._encode_card_onehot(card)
    
    # 2. ì»¤ë®¤ë‹ˆí‹° ì¹´ë“œ (34-118)
    for i, card in enumerate(self.game.community_cards):
        obs[34+i*17:34+(i+1)*17] = self._encode_card_onehot(card)
    
    # ì—†ëŠ” ì¹´ë“œëŠ” all-zero (one-hotì˜ ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„)
    # ë³„ë„ ì²˜ë¦¬ ë¶ˆí•„ìš”!
    
    # === ê²Œì„ ìƒíƒœ (119-149): ì´ì¤‘ ì •ê·œí™” ===
    player = self.game.players[player_id]
    opponent = self.game.players[1 - player_id]
    pot = self.game.get_pot_size()
    to_call = self.game.current_bet - player.bet_this_round
    
    bb = self.big_blind
    max_bb = self.starting_chips / bb  # 500BB
    
    obs[119:150] = [  # ê²Œì„ ìƒíƒœëŠ” 119-149 (31ì°¨ì›)
        (player.chips / bb) / max_bb,
        (opponent.chips / bb) / max_bb,
        (pot / bb) / max_bb,
        (self.game.current_bet / bb) / max_bb,
        (player.bet_this_round / bb) / max_bb,
        (to_call / bb) / max_bb,
        1.0 if self.game.button_position == player_id else 0.0,
        {'preflop': 0.0, 'flop': 0.33, 'turn': 0.66, 'river': 1.0}[self.game.street.value],
        to_call / (pot + to_call) if to_call > 0 and pot > 0 else 0.0,
        np.clip((player.chips / pot) / 10.0, 0, 1.0) if pot > 0 else 1.0,
        self.hand_count / self.max_hands,
        len(self.game.community_cards) / 5.0,
        (self.game.min_raise / bb) / max_bb,
        (opponent.bet_this_round / bb) / max_bb,
        (opponent.bet_this_hand / bb) / max_bb,
        bb / (self.starting_chips / bb)
    ]
    
    # === ì•¡ì…˜ íˆìŠ¤í† ë¦¬ (150-309): ìŠ¤íŠ¸ë¦¬íŠ¸ë³„ ë³´ì¡´ + ë² íŒ… ë¹„ìœ¨ ===
    
    # ê° ìŠ¤íŠ¸ë¦¬íŠ¸ë³„ ì•¡ì…˜ íˆìŠ¤í† ë¦¬ (ìµœê·¼ 4ê°œ)
    street_actions = {
        'preflop': self.action_history.get('preflop', []),
        'flop': self.action_history.get('flop', []),
        'turn': self.action_history.get('turn', []),
        'river': self.action_history.get('river', [])
    }
    
    offset = 150
    for street in ['preflop', 'flop', 'turn', 'river']:
        actions = street_actions[street][-4:]  # ìµœê·¼ 4ê°œ
        
        # ê° ì•¡ì…˜ì„ 10ì°¨ì›ìœ¼ë¡œ ì¸ì½”ë”©
        for i in range(4):
            if i < len(actions):
                action_idx, player_id, bet_ratio = actions[i]
                # Action one-hot (7ì°¨ì›)
                obs[offset + i*10 : offset + i*10 + 7] = np.eye(7)[action_idx]
                # Player one-hot (2ì°¨ì›): [ë‚˜, ìƒëŒ€]
                obs[offset + i*10 + 7 : offset + i*10 + 9] = np.eye(2)[player_id]
                # Bet ratio (1ì°¨ì›): íŒŸ ëŒ€ë¹„ ë² íŒ… ë¹„ìœ¨
                obs[offset + i*10 + 9] = bet_ratio
            # else: all-zero (ì•¡ì…˜ ì—†ìŒ)
        
        offset += 40  # ë‹¤ìŒ ìŠ¤íŠ¸ë¦¬íŠ¸ë¡œ (4 actions Ã— 10 dims)
    
    return obs
```

**ì•¡ì…˜ íˆìŠ¤í† ë¦¬ ì¶”ì  (ì—…ë°ì´íŠ¸)**:
```python
def _record_action(self, action_idx: int, player_id: int, bet_amount: float, pot_before: float):
    """ì•¡ì…˜ì„ í˜„ì¬ ìŠ¤íŠ¸ë¦¬íŠ¸ íˆìŠ¤í† ë¦¬ì— ê¸°ë¡"""
    current_street = self.game.street.value
    
    # ë² íŒ… ë¹„ìœ¨ ê³„ì‚° (íŒŸ ëŒ€ë¹„)
    if pot_before > 0:
        bet_ratio = bet_amount / pot_before
    else:
        bet_ratio = 0.0
    
    # Clip to reasonable range
    bet_ratio = np.clip(bet_ratio, 0.0, 2.5)
    
    self.action_history[current_street].append((action_idx, player_id, bet_ratio))

def _map_bet_to_bucket(self, bet_ratio: float) -> int:
    """
    ë² íŒ… ë¹„ìœ¨ì„ ê°€ì¥ ê°€ê¹Œìš´ ì•¡ì…˜ ë²„í‚·ìœ¼ë¡œ ë§¤í•‘ (Nearest Neighbor)
    
    Args:
        bet_ratio: íŒŸ ëŒ€ë¹„ ë² íŒ… ë¹„ìœ¨ (0.0 ~ 2.5)
    
    Returns:
        action_idx: 2-5 (Bet33%, Bet75%, Bet100%, Bet150%)
    
    ë§¤í•‘ ê·œì¹™:
        - 0.33 ê·¼ì²˜ â†’ 2 (Bet33%)
        - 0.75 ê·¼ì²˜ â†’ 3 (Bet75%)
        - 1.00 ê·¼ì²˜ â†’ 4 (Bet100%)
        - 1.50 ê·¼ì²˜ â†’ 5 (Bet150%)
        - 2.0+ â†’ 6 (All-in)
    """
    # ë²³ ë²„í‚· ì¤‘ì‹¬ê°’
    buckets = [0.33, 0.75, 1.0, 1.5]
    
    # All-in íŠ¹ìˆ˜ ì¼€ì´ìŠ¤
    if bet_ratio >= 2.0:
        return 6
    
    # Euclidean distanceë¡œ ê°€ì¥ ê°€ê¹Œìš´ ë²„í‚· ì°¾ê¸°
    distances = [abs(bet_ratio - bucket) for bucket in buckets]
    nearest_idx = np.argmin(distances)
    
    return nearest_idx + 2  # 2-5 (Bet33%, Bet75%, Bet100%, Bet150%)

# ì˜ˆì‹œ:
# bet_ratio = 0.45
# distances = [|0.45-0.33|=0.12, |0.45-0.75|=0.30, |0.45-1.0|=0.55, |0.45-1.5|=1.05]
# nearest_idx = 0 (0.12ê°€ ìµœì†Œ)
# return 0 + 2 = 2 (Bet33%)
```

**ì‚¬ìš© ì˜ˆì‹œ**:
```python
# ì•¡ì…˜ íˆìŠ¤í† ë¦¬ ê¸°ë¡ ì‹œ
pot_before = self.game.get_pot_size()
bet_amount = 22.5  # ì˜ˆ: íŒŸì´ 50, 22.5 ë² íŒ…
bet_ratio = bet_amount / pot_before  # 0.45

# One-hot ì¹´í…Œê³ ë¦¬ ê²°ì •
action_bucket = self._map_bet_to_bucket(bet_ratio)  # 2 (Bet33%)

# íˆìŠ¤í† ë¦¬ ì €ì¥: (ì¹´í…Œê³ ë¦¬, í”Œë ˆì´ì–´, ì •í™•í•œ ë¹„ìœ¨)
self._record_action(action_bucket, player_id, bet_ratio, pot_before)

**ì•¡ì…˜ íˆìŠ¤í† ë¦¬ ì¶”ì  (ë³„ë„ êµ¬í˜„ í•„ìš”)**:
```python
def __init__(self, ...):
    # ...
    self.action_history = {
        'preflop': [],
        'flop': [],
        'turn': [],
        'river': []
    }

def _record_action(self, action_idx: int, player_id: int):
    """ì•¡ì…˜ì„ í˜„ì¬ ìŠ¤íŠ¸ë¦¬íŠ¸ íˆìŠ¤í† ë¦¬ì— ê¸°ë¡"""
    current_street = self.game.street.value
    self.action_history[current_street].append((action_idx, player_id))

def _start_new_hand(self):
    # ...
    # í•¸ë“œ ì‹œì‘ ì‹œ íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
    self.action_history = {
        'preflop': [],
        'flop': [],
        'turn': [],
        'river': []
    }
```

---

### 2. ì•¡ì…˜ ê³µê°„ (Action Space)

**íƒ€ì…**: `spaces.Discrete(7)`

**ì•¡ì…˜ ì¸ë±ìŠ¤ â†’ POKERENGINE ë§¤í•‘**:

```python
0: Fold       â†’ Action.fold()
1: Check/Call â†’ Action.check() or Action.call(to_call)
2: Bet 33%    â†’ Action.bet/raise_to(pot * 0.33)
3: Bet 75%    â†’ Action.bet/raise_to(pot * 0.75)
4: Bet 100%   â†’ Action.bet/raise_to(pot * 1.0)  # Pot bet
5: Bet 150%   â†’ Action.bet/raise_to(pot * 1.5)  # Overbet
6: All-in     â†’ Action.all_in(chips)
```

**ë§¤í•‘ ë¡œì§** (NumPyë¡œ ê³„ì‚°):
```python
def _map_action(self, action_idx: int, player_id: int) -> Action:
    player = self.game.players[player_id]
    pot = self.game.get_pot_size()
    to_call = self.game.current_bet - player.bet_this_round
    
    if action_idx == 0:
        return Action.fold()
    elif action_idx == 1:
        return Action.check() if to_call == 0 else Action.call(to_call)
    elif action_idx == 6:
        return Action.all_in(player.chips)
    else:
        # Percentage bets (2-5)
        pcts = np.array([0.33, 0.75, 1.0, 1.5])
        pct = pcts[action_idx - 2]
        bet_amount = pot * pct
        
        if self.game.current_bet > 0:
            # Raise
            target = max(
                self.game.current_bet + bet_amount,
                self.game.current_bet + self.game.min_raise
            )
            max_bet = player.chips + player.bet_this_round
            return Action.all_in(player.chips) if target > max_bet else Action.raise_to(target)
        else:
            # Bet
            bet_amount = max(bet_amount, self.big_blind)
            return Action.all_in(player.chips) if bet_amount > player.chips else Action.bet(bet_amount)
```

---

### 3. ë³´ìƒ í•¨ìˆ˜ (Reward Function)

**íƒ€ì…**: Dense Reward (ë§¤ í•¸ë“œë§ˆë‹¤)

**ê³µì‹** (NumPy í´ë¦¬í•‘):
```python
def _calculate_reward(self, player_id: int, stack_before: float, stack_after: float) -> float:
    chip_change = stack_after - stack_before
    bb_change = chip_change / self.big_blind
    reward = bb_change / 100.0  # Normalization factor
    return float(np.clip(reward, -5.0, 5.0))
```

**íŠ¹ì§•**:
- ì¹© EV ìµœëŒ€í™” = ìµœì  í¬ì»¤ ì „ëµ
- ë²”ìœ„: -1.0 ~ +1.0 (ì¼ë°˜ì )
- í´ë¦¬í•‘ìœ¼ë¡œ ê·¹ë‹¨ê°’ ë°©ì§€
    legal = self.game.get_legal_actions(self.game.get_current_player())
    mask = np.zeros(7, dtype=np.int8)
    
    if ActionType.FOLD in legal: mask[0] = 1
    if ActionType.CHECK in legal or ActionType.CALL in legal: mask[1] = 1
    if ActionType.BET in legal or ActionType.RAISE in legal: mask[2:6] = 1
    if ActionType.ALL_IN in legal: mask[6] = 1
    
    return mask
```

---

### 5. ì—í”¼ì†Œë“œ êµ¬ì¡°

**Tournament ë°©ì‹**:
- ì—í”¼ì†Œë“œ = í•œ í† ë„ˆë¨¼íŠ¸ (í•œ ëª…ì˜ ì¹©ì´ 0ì´ ë  ë•Œê¹Œì§€)
- ì‹œì‘ ì¹©: 1000 (ë¸”ë¼ì¸ë“œ 1/2 ê¸°ì¤€ 500BB)
- ìµœëŒ€ í•¸ë“œ: 500 (ë¬´í•œ ë£¨í”„ ë°©ì§€)
- ë”œëŸ¬ ë¡œí…Œì´ì…˜: ë§¤ í•¸ë“œ êµëŒ€

**í•¸ë“œ êµ¬ì¡°**:
- ë”œëŸ¬ ë¡œí…Œì´ì…˜ í›„ `game.start_hand(chips, button)` í˜¸ì¶œ
- í•¸ë“œ ì¢…ë£Œ ì‹œ ë² íŒ… ë¦¬ì…‹, ì¹© ëˆ„ì 
- POKERENGINEì´ ìë™ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬íŠ¸ ì§„í–‰

---

### 6. NumPy ìµœì í™” íŒ

```python
# âœ… Good: Pre-allocate
obs = np.zeros(60, dtype=np.float32)

# âœ… Good: Vectorized slicing
obs[14:30] = state_features

# âœ… Good: NumPy clip
reward = np.clip(raw_reward, -5.0, 5.0)

# âŒ Bad: List append + convert
obs = []
obs.append(...)
obs = np.array(obs)  # Slow!
```

---


---

## ğŸ§  ëª¨ë¸ ì•„í‚¤í…ì²˜


### Phase 1: FC + LSTM (ì‹œì‘) - ê¶Œì¥!

> [!WARNING] **Transformer ì „í™˜ ì‹ ì¤‘ë¡ **
> TransformerëŠ” ê°•ë ¥í•˜ì§€ë§Œ ì´ˆê¸° ë‹¨ê³„ì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì´ìœ ë¡œ **ë¹„ê¶Œì¥**ë©ë‹ˆë‹¤:
> 1. **ë°ì´í„° íš¨ìœ¨ì„±**: MLP/LSTM ëŒ€ë¹„ 5~10ë°° ë§ì€ ë°ì´í„° í•„ìš”
> 2. **ì¶”ë¡  ì†ë„**: ì‹œë®¬ë ˆì´ì…˜(Rollout) ì†ë„ ì €í•˜ â†’ ì „ì²´ í•™ìŠµ ì†ë„ ê°ì†Œ
> 3. **ì§§ì€ ì»¨í…ìŠ¤íŠ¸**: í¬ì»¤ íˆìŠ¤í† ë¦¬(10~20)ëŠ” LSTMìœ¼ë¡œ ì¶©ë¶„íˆ ì»¤ë²„ ê°€ëŠ¥
> 
> **ê²°ë¡ **: ì´ˆê¸°ì—ëŠ” **FC + LSTM**ì— ì§‘ì¤‘í•˜ê³ , TransformerëŠ” Phase 3 ì´í›„ ì‹¤í—˜ì ìœ¼ë¡œ ê³ ë ¤í•˜ì‹­ì‹œì˜¤.

**âš ï¸ ì¤‘ìš”**: ìˆœìˆ˜ MLPëŠ” ì‹œí€€ìŠ¤ ì´í•´ ë¶ˆê°€

**MLPì˜ í•œê³„**:
```python
# MLPê°€ ë³´ëŠ” ë°©ì‹:
obs = [action1, action2, ..., action_n]
â†’ "310ê°œì˜ ë…ë¦½ ë³€ìˆ˜" (ìˆœì„œ ë¬´ì‹œ!)

# MLPê°€ ëª» ë³´ëŠ” ê²ƒ:
"Preflop: ì¡°ì‹¬ìŠ¤ëŸ½ê²Œ call â†’ Flop: ë¹ ë¥´ê²Œ raise"
â†’ ì‹œê°„ì  íŒ¨í„´ í¬ì°© ë¶ˆê°€
```

**í•´ê²°ì±…: FC (Feature Extraction) + LSTM (Temporal)**

**âœ… ì˜¬ë°”ë¥¸ êµ¬ì¡°**:
```
ì…ë ¥ (310) - One-hot ì¹´ë“œ + ê²Œì„ ìƒíƒœ + ì•¡ì…˜ íˆìŠ¤í† ë¦¬
  â†’ FC(256) + ReLU  [íŠ¹ì§• ì¶”ì¶œ: "ì´ íŒ¨ëŠ” ê°•í•˜ë‹¤"]
  â†’ FC(256) + ReLU  [íŠ¹ì§• ì••ì¶•: 310 â†’ 256]
  â†’ LSTM(256)       [ì‹œí€€ìŠ¤ ì´í•´: "íŒ¨í„´ íŒŒì•…"]
  â†’ FC(7)           [ì•¡ì…˜ í™•ë¥ ]
```

**ì™œ FCê°€ ë¨¼ì €?**
```python
# âŒ ì˜ëª»ëœ ìˆœì„œ
Input(310) â†’ LSTM(256) â†’ FC
ë¬¸ì œ:
- 310ì°¨ì› sparse inputì„ LSTM ì§ì ‘ = ëŠë¦¼
- ì—°ì‚°ëŸ‰ í­ë°œ
- ìˆ˜ë ´ ì–´ë ¤ì›€

# âœ… ì˜¬ë°”ë¥¸ ìˆœì„œ
Input(310) â†’ FC(256) â†’ LSTM(256)
ì¥ì :
- FCê°€ íŠ¹ì§• ì¶”ì¶œ: One-hot â†’ Abstract features
- LSTMì€ ì••ì¶•ëœ íŠ¹ì§•ì˜ ì‹œê°„ì  íë¦„ë§Œ ì²˜ë¦¬
- íš¨ìœ¨ì ì´ê³  ë¹ ë¦„
```

**RLlib êµ¬í˜„** (ìë™ FC ì¶”ê°€!):
            "fcnet_hiddens": [256, 256],  # â­ FC(256) â†’ FC(256)
            "fcnet_activation": "relu",
            
            # 2. LSTM ì„¤ì •
            "use_lstm": True,              # â­ LSTM í™œì„±í™”
            "lstm_cell_size": 256,         # Hidden state í¬ê¸°
            
            # 3. ì‹œí€€ìŠ¤ ê¸¸ì´
            #    í¬ì»¤ í•œ í•¸ë“œ = ë³´í†µ 10~30 ì•¡ì…˜
            #    20ì´ë©´ ì¶©ë¶„í•˜ê³  íš¨ìœ¨ì 
            "max_seq_len": 20,
            
            # 4. ì´ì „ ì •ë³´ í™œìš©
            "lstm_use_prev_action": True,   # ì´ì „ ì•¡ì…˜ ì…ë ¥ì— ì¶”ê°€
            "lstm_use_prev_reward": True    # ì´ì „ ë³´ìƒë„ ì¶”ê°€ (Sparseì§€ë§Œ ìœ ìš©)
        }
    )
)
```

**ì‹¤ì œ êµ¬ì¡° (RLlib ìë™ ìƒì„±)**:
```
Input(310)
  â†“
FC(310 â†’ 256) + ReLU  [fcnet_hiddens[0]]
  â†“
FC(256 â†’ 256) + ReLU  [fcnet_hiddens[1]]
  â†“
LSTM(256)  [use_lstm=True]
  â†“  
Policy Head: FC(256 â†’ 7)   [ì•¡ì…˜ í™•ë¥ ]
Value Head:  FC(256 â†’ 1)   [ìƒíƒœ ê°€ì¹˜]
```

**ğŸ’¡ ë‚´ë¶€ ì‘ë™ ì›ë¦¬** (íŠ¹ì§• ì¶”ì¶œ ê³¼ì •)

**ë°ì´í„° ë³€í™˜ ë‹¨ê³„**:
```
Step 1: Raw Input (310ì°¨ì›)
[0,1,0,0,0,...,0.75,0.45,...]
â†“ "ì´í•´í•˜ê¸° í˜ë“  0ê³¼ 1ì˜ ë‚˜ì—´"

Step 2: FC Layer 1 (256ì°¨ì›)
[0.23, 0.87, 0.12, ...]
â†“ "1ì°¨ íŒ¨í„´ ì¸ì‹"
AI ë‚´ë¶€: "Aì™€ Kê°€ ìˆë„¤? ìŠ¤í˜ì´ë“œê°€ 3ì¥?"

Step 3: FC Layer 2 (256ì°¨ì›)  
[0.91, 0.15, 0.68, ...]
â†“ "ì¶”ìƒì  íŠ¹ì§• ì™„ì„±"
AI ë‚´ë¶€: 
- ê°•ë„:0.91, ìœ„í—˜:0.15, íŒŸì˜¤ì¦ˆ:0.68
â†’ **310ê°œ â†’ 256ê°œ 'ìƒí™© ìš”ì•½ ë²¡í„°'**

Step 4: LSTM
ì‹œê°„ì¶• ì²˜ë¦¬ + ë©”ëª¨ë¦¬
AI ë‚´ë¶€:
"Preflop ê°•í–ˆëŠ”ë° â†’ Flop ìœ„í—˜ ì¦ê°€ â†’ Turn ë” ìœ„í—˜"
â†’ "ìƒí™© ì•…í™” ì¤‘!"
```

**êµ¬ì²´ì  ì˜ˆì‹œ**:
```python
# Raw Input (310ì°¨ì›)
[
  0,0,1,0,...,0,  # â™ 3 (one-hot)
  1,0,0,0,...,0,  # â™ 2
  0.95,           # My stack
  0.25,           # Pot
  ...
  0.75,           # Bet ratio
]
â†“
# FC1 í•™ìŠµ íŒ¨í„´:
"ìŠ¤í˜ì´ë“œ ì—¬ëŸ¬ ê°œ â†’ Flush ê°€ëŠ¥ì„±"
"Bet 0.75 ë°˜ë³µ â†’ ê³µê²©ì  ìŠ¤íƒ€ì¼"
â†“
# FC2 ì¶”ìƒí™”:
feature[0] = 0.91  # "ë‚´ í•¸ë“œ ê°•ë„"
feature[1] = 0.15  # "ë³´ë“œ ìœ„í—˜ë„"  
feature[2] = 0.33  # "ìƒëŒ€ ê³µê²©ì„±"
feature[3] = 0.68  # "íŒŸ ì˜¤ì¦ˆ"
â†“
# LSTM ì‹œí€€ìŠ¤:
t-3: [ê°•:0.9, ìœ„:0.2] "ì•ˆì „"
t-2: [ê°•:0.9, ìœ„:0.5] "ì¡°ê¸ˆ ìœ„í—˜"
t-1: [ê°•:0.7, ìœ„:0.8] "ìœ„í—˜"
now: [ê°•:0.6, ìœ„:0.9] "ë§¤ìš° ìœ„í—˜!"
â†’ "í´ë“œ ê³ ë ¤"
```

**ì™œ FC 2ê°œ?**
- **FC1**: ë‹¨ìˆœ íŒ¨í„´ ("ì´ ìœ„ì¹˜=Ace")
- **FC2**: ë³µí•© íŒ¨í„´ ("AK+Flush+ìƒëŒ€ì•½í•¨=ê³µê²©")
  â†’ ì¶”ìƒì  *í¬ì»¤ ê°œë…* í˜•ì„±


**ì¥ì **:
- âœ… **íš¨ìœ¨ì **: FCê°€ ì°¨ì› ì¶•ì†Œ (310 â†’ 256)
- âœ… **íŠ¹ì§• ì¶”ì¶œ**: One-hot â†’ ì¶”ìƒì  íŠ¹ì§•
- âœ… **ì‹œí€€ìŠ¤ ì´í•´**: LSTMì´ temporal pattern í•™ìŠµ
- âœ… **ê°€ë²¼ì›€**: Transformerë³´ë‹¤ í›¨ì”¬ ì‘ìŒ
- âœ… **RLlib ìë™í™”**: `fcnet_hiddens`ë¡œ ìë™ êµ¬ì„±
- âœ… **í¬ì»¤ ìµœì **: ì•¡ì…˜ ì‹œí€€ìŠ¤ + íŠ¹ì§• ì¡°í•©

**íŒŒë¼ë¯¸í„° ìˆ˜**: ~600K
- FC layers: ~300K
- LSTM: ~250K
- Heads: ~50K

**ì‹œí€€ìŠ¤ ì²˜ë¦¬ ì˜ˆì‹œ**:
```python
# FCê°€ ì¶”ì¶œí•œ íŠ¹ì§•:
feature_t1 = [ê°•í•œíŒ¨: 0.9, ìœ„í—˜ë³´ë“œ: 0.3, ...]
feature_t2 = [ê°•í•œíŒ¨: 0.9, ìœ„í—˜ë³´ë“œ: 0.7, ...]
feature_t3 = [ê°•í•œíŒ¨: 0.6, ìœ„í—˜ë³´ë“œ: 0.9, ...]

# LSTMì´ ì´í•´:
"ì²˜ìŒì—” ê°•í–ˆì§€ë§Œ ì ì  ì•½í•´ì§" â†’ Bluff ê°€ëŠ¥ì„± ê°ì§€
```

---

### ğŸ® Inference ì‹œ State ê´€ë¦¬ (ì¤‘ìš”!)

**MLP vs LSTM ì°¨ì´**:

```python
# MLP (Stateless):
action = algo.compute_single_action(obs)
# ê°„ë‹¨! ìƒíƒœ ê´€ë¦¬ ë¶ˆí•„ìš”

# LSTM (Stateful):
# í•¸ë“œ ì‹œì‘ ì‹œ ì´ˆê¸°í™”
state = algo.get_initial_state()  
# ë˜ëŠ”
state = [np.zeros([256], np.float32), np.zeros([256], np.float32)]

# ë§¤ í„´ë§ˆë‹¤
action, state, _ = algo.compute_single_action(
    obs, 
    state=state  # â­ ì´ì „ state ì „ë‹¬!
)
# stateë¥¼ ì—…ë°ì´íŠ¸í•˜ê³  ë‹¤ìŒ í„´ì— ì¬ì‚¬ìš©

# í•¸ë“œ ì¢…ë£Œ ì‹œ state ë¦¬ì…‹
```

**ì‹¤ì „ ì˜ˆì‹œ** (`play_vs_ai.py`):
```python
# í•¸ë“œ ì‹œì‘
lstm_state_0 = algo.get_initial_state()  # P0 ìƒíƒœ
lstm_state_1 = algo.get_initial_state()  # P1 ìƒíƒœ

while not hand_over:
    current_player = game.get_current_player()
    obs = env._get_observation(current_player)
    
    if current_player == 0:
        action, lstm_state_0, _ = algo.compute_single_action(
            obs, state=lstm_state_0
        )
    else:
        action, lstm_state_1, _ = algo.compute_single_action(
            obs, state=lstm_state_1
        )
    
    game.process_action(current_player, action)

# ë‹¤ìŒ í•¸ë“œ: state ë¦¬ì…‹!
```

**ì£¼ì˜ì‚¬í•­**:
- âœ… í•¸ë“œë§ˆë‹¤ state ì´ˆê¸°í™”
- âœ… í”Œë ˆì´ì–´ë³„ë¡œ state ë¶„ë¦¬
- âœ… stateëŠ” LSTM hidden state (2ê°œ tensor)


### ğŸ›¡ï¸ Action Masking (í•„ìˆ˜!)

**ì¤‘ìš”**: Action Maskingì€ **ì„ íƒì´ ì•„ë‹Œ í•„ìˆ˜**

**ë¬¸ì œ - ë¶ˆë²• ì•¡ì…˜ ê°•ì œ ë³€í™˜ ë°©ì‹**:
```python
# âŒ ë‚˜ìœ ë°©ë²• (í˜„ì¬ ê³„íš)
if not legal:
    action = check_or_call  # ê°•ì œ ë³€í™˜

ë¬¸ì œì :
1. ì‹ ê²½ë§ì´ ê³„ì† ë¶ˆë²• ì•¡ì…˜ í•™ìŠµ
2. í™•ë¥  ë¶„í¬ ì™œê³¡ (Fold 30% â†’ Checkë¡œ ë³€í™˜)
3. í•™ìŠµ ë¹„íš¨ìœ¨
4. ìˆ˜ë ´ ëŠë¦¼
```

**í•´ê²° - Action Masking**:
```python
# âœ… ì˜¬ë°”ë¥¸ ë°©ë²•
legal_mask = [1, 1, 0, 1, 1, 0, 1]  # 0 = ë¶ˆë²•
logits[~legal_mask] = -inf
probs = softmax(logits)

ì¥ì :
1. ë¶ˆë²• ì•¡ì…˜ í™•ë¥  = 0
2. í•©ë²• ì•¡ì…˜ë§Œ í•™ìŠµ
3. ì•ˆì •ì  í•™ìŠµ
4. ë¹ ë¥¸ ìˆ˜ë ´
```

**RLlib êµ¬í˜„ - ParametricActionModel**:

í™˜ê²½ì—ì„œ ë§ˆìŠ¤í¬ ì œê³µ:
```python
# env.py
def step(self, action):
    obs = self._get_observation(...)
    info = {
        'action_mask': self._get_legal_actions_mask()  # í•„ìˆ˜!
    }
    return obs, reward, terminated, truncated, info

def _get_legal_actions_mask(self) -> np.ndarray:
    """7ì°¨ì› binary mask"""
    legal = self.game.get_legal_actions(self.game.get_current_player())
    mask = np.zeros(7, dtype=np.int8)
    
    if ActionType.FOLD in legal: mask[0] = 1
    if ActionType.CHECK in legal or ActionType.CALL in legal: mask[1] = 1
    if ActionType.BET in legal or ActionType.RAISE in legal: mask[2:6] = 1
    if ActionType.ALL_IN in legal: mask[6] = 1
    
    return mask
```

ì»¤ìŠ¤í…€ ëª¨ë¸ (RLlib ParametricActionModel):
```python
# models/masked_mlp.py
from ray.rllib.models.torch.torch_modelv2 import TorchModelV2
from ray.rllib.utils.torch_utils import FLOAT_MIN
import torch.nn as nn

class MaskedMLP(TorchModelV2, nn.Module):
    def __init__(self, obs_space, action_space, num_outputs, model_config, name):
        TorchModelV2.__init__(self, obs_space, action_space, num_outputs, model_config, name)
        nn.Module.__init__(self)
        
        self.fc1 = nn.Linear(310, 512)
        self.fc2 = nn.Linear(512, 512)
        self.fc3 = nn.Linear(512, 256)
        self.logits = nn.Linear(256, 7)
        self.value = nn.Linear(256, 1)
        
    def forward(self, input_dict, state, seq_lens):
        obs = input_dict["obs"]
        action_mask = input_dict["obs"]["action_mask"]  # ë§ˆìŠ¤í¬ ì¶”ì¶œ
        
        # Forward pass
        x = torch.relu(self.fc1(obs))
        x = torch.relu(self.fc2(x))
        x = torch.relu(self.fc3(x))
        logits = self.logits(x)
        
        # â­ Action Masking ì ìš©
        inf_mask = torch.clamp(torch.log(action_mask), FLOAT_MIN, FLOAT_MIN)
        masked_logits = logits + inf_mask
        
        self._value = self.value(x).squeeze(1)
        return masked_logits, state
    
    def value_function(self):
        return self._value
```

RLlib ì„¤ì •:
```python
# train.py
from ray.rllib.algorithms.ppo import PPOConfig
from models.masked_mlp import MaskedMLP

config = (
    PPOConfig()
    .training(
        model={
            "custom_model": MaskedMLP,  # ì»¤ìŠ¤í…€ ëª¨ë¸ ì‚¬ìš©
        }
    )
)
```

**âš ï¸ ì¤‘ìš”: step()ì—ì„œ ì ˆëŒ€ ê¸ˆì§€!**

```python
# âŒ ì ˆëŒ€ í•˜ì§€ ë§ ê²ƒ! (ì‚¬í›„ ì²˜ë¦¬)
def step(self, action_dict):
    success, error = self.game.process_action(player, action)
    
    if not success:
        # âŒ ê°•ì œ ë³€í™˜ ê¸ˆì§€!
        action = Action.check()  
        self.game.process_action(player, action)
        
    # ì´ ë°©ì‹ì€:
    # 1. ì‹ ê²½ë§ì´ í•™ìŠµ ëª» í•¨
    # 2. í™•ë¥  ë¶„í¬ ì™œê³¡
    # 3. ë¹„íš¨ìœ¨ì 
```

**âœ… ì˜¬ë°”ë¥¸ step() êµ¬í˜„**:

```python
def step(self, action_dict):
    current_player = self.game.get_current_player()
    action = action_dict[f"player_{current_player}"]
    
    # â­ Action Maskingìœ¼ë¡œ ì´ë¯¸ í•©ë²• ì•¡ì…˜ë§Œ ì˜´
    # ê·¸ëƒ¥ ì‹¤í–‰!
    engine_action = self._map_action(action, current_player)
    self.game.process_action(current_player, engine_action)
    
    # ì‚¬í›„ ì²˜ë¦¬ ì—†ìŒ!
    # ë§ˆìŠ¤í‚¹ì´ ì œëŒ€ë¡œ ë˜ì—ˆë‹¤ë©´ í•­ìƒ ì„±ê³µ!
    
    # (ë””ë²„ê¹…ìš©ìœ¼ë¡œë§Œ ì²´í¬)
    # assert success, "Action masking failed!"
```

**í•µì‹¬**:
- âœ… í™˜ê²½: `action_mask`ë¥¼ infoì— ì œê³µ
- âœ… ëª¨ë¸: Logit ë ˆë²¨ì—ì„œ ë§ˆìŠ¤í‚¹ ì ìš©
- âœ… step(): ë°›ì€ ì•¡ì…˜ì„ ê·¸ëŒ€ë¡œ ì‹¤í–‰
- âŒ **ì ˆëŒ€**: ì‚¬í›„ ì²˜ë¦¬ ê¸ˆì§€!


**ì°¸ê³  ìë£Œ**:
- RLlib Parametric Actions: https://docs.ray.io/en/latest/rllib/rllib-models.html#parametric-action-space
- Action Masking ì˜ˆì œ: https://github.com/ray-project/ray/blob/master/rllib/examples/action_masking.py


---

### ğŸ’° ë³´ìƒ í•¨ìˆ˜ (Sparse Reward) - í•„ìˆ˜ ì£¼ì˜!

**âš ï¸ ìš©ì–´ ì •ì •**: **Sparse Reward** (Dense ì•„ë‹˜!)

**ì¤‘ìš”í•œ êµ¬ë¶„**:
- âŒ Dense Reward = ë§¤ ìŠ¤í…ë§ˆë‹¤ ë³´ìƒ (ìš°ë¦¬ëŠ” X)
- âœ… Sparse Reward = í•¸ë“œ ì¢…ë£Œ ì‹œì—ë§Œ ë³´ìƒ (ìš°ë¦¬!)

**ì ˆëŒ€ ì›ì¹™**:
```python
# âœ… ì˜¬ë°”ë¥¸ êµ¬í˜„
í•¸ë“œ ì§„í–‰ ì¤‘ (betting, calling, raising):
    reward = 0.0  # ì ˆëŒ€ ë³´ìƒ ì—†ìŒ!
    
í•¸ë“œ ì¢…ë£Œ ì‹œ (showdown or all folded):
    reward = (stack_after - stack_before) / BB / 100.0
```

**ìœ„í—˜í•œ ì°©ê° - ì ˆëŒ€ ê¸ˆì§€!**:
```python
# âŒ ì ˆëŒ€ í•˜ë©´ ì•ˆ ë˜ëŠ” ê²ƒë“¤
if action == BET:
    reward = -bet_amount  # ë² íŒ… = ì¦‰ì‹œ ì†ì‹¤?
    â†’ AIê°€ ë² íŒ… íšŒí”¼ í•™ìŠµ! (ì²´í¬ë§Œ í•¨)

if action == FOLD:
    reward = -(chips_invested)  # í´ë“œ = ì†ì‹¤ í™•ì •?
    â†’ AIê°€ ë¬´ì¡°ê±´ í´ë“œ í•™ìŠµ!
    
if action == CALL:
    reward = -to_call  # ì½œ = ëˆ ë‚˜ê°?
    â†’ AIê°€ í´ë“œë§Œ í•¨!

# í•¸ë“œ ì¤‘ê°„ì—ëŠ” ë¬´ì¡°ê±´ reward = 0.0!
```

**ì˜¬ë°”ë¥¸ êµ¬í˜„**:
```python
def _calculate_reward(self, player_id: int, stack_before: float, stack_after: float) -> float:
    """
    í•¸ë“œ ì¢…ë£Œ ì‹œì—ë§Œ í˜¸ì¶œ!
    
    ë³´ìƒ = ìµœì¢… ì¹© ë³€í™”ëŸ‰ (BB ì •ê·œí™”)
    - ì´ê¸´ ê²½ìš°: +ì¹© â†’ ì–‘ìˆ˜ ë³´ìƒ
    - ì§„ ê²½ìš°: -ì¹© â†’ ìŒìˆ˜ ë³´ìƒ
    - ë¬´ìŠ¹ë¶€: 0ì¹© â†’ 0 ë³´ìƒ
    """
    chip_change = stack_after - stack_before
    bb_change = chip_change / self.big_blind
    
    # â­ ì •ê·œí™”: 100BB ê¸°ì¤€ (ê°•í•œ í•™ìŠµ ì‹ í˜¸)
    # ìŠ¤íƒ ë¶„í¬: 5-250BB
    # ì¼ë°˜ì  ì†ìµ: Â±100BB
    # ì •ê·œí™”: bb_change / 100
    reward = bb_change / 100.0
    
    # ë³´ìƒ ë²”ìœ„: [-2.5, +2.5]
    # +250BB â†’ +2.5 (ê·¹ë‹¨ì  ìŠ¹ë¦¬)
    # +100BB â†’ +1.0 (í° ìŠ¹ë¦¬) âœ¨
    # +50BB â†’ +0.5 (ì¼ë°˜ì  ìŠ¹ë¦¬)
    # -50BB â†’ -0.5 (ì¼ë°˜ì  ì†ì‹¤)
    # -100BB â†’ -1.0 (í° ì†ì‹¤)
    # -250BB â†’ -2.5 (ê·¹ë‹¨ì  ì†ì‹¤)
    
    return float(reward)
    
# âš ï¸ ì™œ 100ìœ¼ë¡œ ë‚˜ëˆ„ëŠ”ê°€?
# 
# 250ìœ¼ë¡œ ë‚˜ëˆŒ ë•Œ:
#   +100BB â†’ +0.4 (ì•½í•œ ì‹ í˜¸)
#   +250BB â†’ +1.0
#   ë²”ìœ„: [-1.0, +1.0] (ê¹”ë”í•˜ì§€ë§Œ ì•½í•¨)
#
# 100ìœ¼ë¡œ ë‚˜ëˆŒ ë•Œ:
#   +100BB â†’ +1.0 (ê°•í•œ ì‹ í˜¸!) âœ¨
#   +250BB â†’ +2.5
#   ë²”ìœ„: [-2.5, +2.5]
#   
# ì¥ì :
#   1. ë” ê°•í•œ í•™ìŠµ ì‹ í˜¸ (100BB = 1.0)
#   2. PPOê°€ [-2.5, +2.5] ì¶©ë¶„íˆ ì²˜ë¦¬
#   3. ì§ê´€ì  (100BB = 1.0 ë³´ìƒ)
#   4. ì¼ë°˜ì  ìŠ¹ë¦¬ê°€ ë” ëª…í™•

# Multi-Agent step()ì—ì„œ ì‚¬ìš©:
def step(self, action_dict):
    # ... ì•¡ì…˜ ì²˜ë¦¬ ...
    
    reward_dict = {}
    
    if self.game.is_hand_over:
        # â­ Zero-Sum ë³´ì¥: P0 ë³´ìƒë§Œ ê³„ì‚°, P1ì€ ìŒìˆ˜ ì‚¬ìš©
        stack_before_p0 = self.hand_start_stacks[0]
        stack_after_p0 = self.chips[0]
        
        # P0 ë³´ìƒ ê³„ì‚°
        chip_change = stack_after_p0 - stack_before_p0
        bb_change = chip_change / self.big_blind
        p0_reward = bb_change / 100.0  # ê°•í•œ í•™ìŠµ ì‹ í˜¸
        
        # â­ ì™„ë²½í•œ Zero-Sum ë³´ì¥
        reward_dict = {
            "player_0": float(p0_reward),
            "player_1": float(-p0_reward)  # ì •í™•íˆ ìŒìˆ˜!
        }
        
        # Zero-Sum ê²€ì¦ (ë””ë²„ê¹…ìš©)
        # assert abs(p0_reward + (-p0_reward)) < 1e-10, "Zero-Sum violation!"
        
    else:
        # â­ í•¸ë“œ ì§„í–‰ ì¤‘: ë³´ìƒ ì—†ìŒ!
        next_player = self.game.get_current_player()
        reward_dict[f"player_{next_player}"] = 0.0
    
    return obs_dict, reward_dict, done_dict, truncated_dict, info_dict
```

**ë³´ìƒ ê³„ì‚° ì˜ˆì‹œ**:
```
í•¸ë“œ ì‹œì‘:
- P0 stack: 1000
- P1 stack: 1000
- BB: 2

ì•¡ì…˜ ì‹œí€€ìŠ¤:
1. P0 raises 10    â†’ reward_P0 = 0.0 (ì§„í–‰ ì¤‘)
2. P1 calls 10     â†’ reward_P1 = 0.0 (ì§„í–‰ ì¤‘)
3. (Flop)
4. P0 bets 20      â†’ reward_P0 = 0.0 (ì§„í–‰ ì¤‘)
5. P1 raises 50    â†’ reward_P1 = 0.0 (ì§„í–‰ ì¤‘)
6. P0 folds        â†’ reward_P0 = 0.0 (ì•„ì§!)

í•¸ë“œ ì¢…ë£Œ:
- P0 stack: 940  (lost 60)
  â†’ reward_P0 = (940-1000)/2/100 = -60/2/100 = -0.30
- P1 stack: 1060 (won 60)
  â†’ reward_P1 = (1060-1000)/2/100 = +60/2/100 = +0.30
  
Zero-sum check: -0.30 + 0.30 = 0.0 âœ…
```

**íŠ¹ì§•**:
- âœ… ì¹© EV ìµœëŒ€í™” = í¬ì»¤ ìµœì  ì „ëµ
- âœ… Zero-sum (P0 + P1 = 0)
- âœ… BB ì •ê·œí™”ë¡œ ìŠ¤ì¼€ì¼ ì¼ê´€ì„±
- âœ… í´ë¦¬í•‘ìœ¼ë¡œ í•™ìŠµ ì•ˆì •ì„±
- âœ… ì¤‘ê°„ ë³´ìƒ ì—†ìŒ = ì •í™•í•œ í•™ìŠµ

**ì •ê·œí™” íŒŒë¼ë¯¸í„°**:
- `normalization = 100.0` (ì´ˆê¸°ê°’)
- â†’ Â±100BB ë³€í™” = Â±1.0 ë³´ìƒ
- í•„ìš”ì‹œ ì¡°ì •: 50.0 (ë¯¼ê°) or 200.0 (ë‘”ê°)


### Phase 2: Transformer (ê³ ê¸‰)

**êµ¬ì¡°**:
```
ì…ë ¥ (310) - One-hot ì¹´ë“œ + ê²Œì„ ìƒíƒœ + ì™„ì „í•œ ì•¡ì…˜ íˆìŠ¤í† ë¦¬
  â†’ Positional Encoding
  â†’ Transformer Encoder (4 layers, 8 heads)
  â†’ FC(128)
  â†’ FC(7)
```

**ì¥ì **:
- ì‹œí€€ìŠ¤ ì •ë³´ í™œìš© (ì•¡ì…˜ íˆìŠ¤í† ë¦¬!)
- Attentionì„ í†µí•œ ì˜ì‚¬ê²°ì • í•´ì„
- ë³µì¡í•œ íŒ¨í„´ í•™ìŠµ
- Preflop â†’ River ìŠ¤í† ë¦¬ ì´í•´
- **ë¯¸ë¬˜í•œ Bet Sizing Pattern í•™ìŠµ**

**ì „í™˜ ì‹œê¸°**: MLPë¡œ ê¸°ë³¸ í•™ìŠµ í™•ì¸ í›„

---

## ğŸ“ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜

### PPO (Proximal Policy Optimization)

**ì„ íƒ ì´ìœ **:
- ì•ˆì •ì ì¸ í•™ìŠµ
- Self-playì— ì í•©
- ì—°ì†ì ì¸ ì •ì±… ê°œì„ 
- RLlibì—ì„œ ì˜ ì§€ì›ë¨

**í•˜ì´í¼íŒŒë¼ë¯¸í„° (ì´ˆê¸°ê°’)**:
```python
gamma = 0.99              # í• ì¸ìœ¨
lambda_ = 0.95            # GAE lambda
clip_param = 0.2          # PPO clip ë²”ìœ„
lr = 3e-4                 # í•™ìŠµë¥ 
train_batch_size = 16384  # ë°°ì¹˜ í¬ê¸° (í¬ì»¤ ë¶„ì‚° ê³ ë ¤, ë§¤ìš° í° ë°°ì¹˜ í•„ìˆ˜!)
num_sgd_iter = 10         # SGD ë°˜ë³µ
entropy_coeff = 0.01      # íƒí—˜ ì¸ì„¼í‹°ë¸Œ
```

**í¬ì»¤ íŠ¹í™” ê³ ë ¤ì‚¬í•­**:

**ë°°ì¹˜ í¬ê¸° (16384+)**:
- í¬ì»¤ëŠ” **ë†’ì€ ë¶„ì‚°(Variance)** ê²Œì„
  - ì˜¬ë°”ë¥¸ í”Œë ˆì´ â†’ ì§ˆ ìˆ˜ ìˆìŒ (ìš´ ë‚˜ì¨)
  - ì˜ëª»ëœ í”Œë ˆì´ â†’ ì´ê¸¸ ìˆ˜ ìˆìŒ (ìš´ ì¢‹ìŒ)
- ì‘ì€ ë°°ì¹˜ (8192 ì´í•˜):
  - "ìš´ ì¢‹ê²Œ ì´ê¸´ ë‚˜ìœ í”Œë ˆì´"ë¥¼ í•™ìŠµí•  ìœ„í—˜
  - ë…¸ì´ì¦ˆê°€ ë§ì€ ê·¸ë˜ë””ì–¸íŠ¸
  - ë¶ˆì•ˆì •í•œ í•™ìŠµ
- **í° ë°°ì¹˜ (16384+)**:
  - âœ… ë¶„ì‚°ì´ í‰ê· í™”ë¨ (Law of Large Numbers)
  - âœ… ì§„ì§œ ì‹¤ë ¥ì´ ë“œëŸ¬ë‚¨
  - âœ… ì•ˆì •ì ì¸ ê·¸ë˜ë””ì–¸íŠ¸
  - âœ… ì˜¬ë°”ë¥¸ ì „ëµ í•™ìŠµ

**ê¶Œì¥ ë°°ì¹˜ í¬ê¸°**:
- ì´ˆê¸° í•™ìŠµ: 16384 (ì•ˆì •ì„± ìš°ì„ )
- ì¶©ë¶„í•œ ë°ì´í„° í›„: 32768 (ë” ì•ˆì •ì )
- ë¦¬ì†ŒìŠ¤ ë¶€ì¡± ì‹œ ìµœì†Œ: 16384 (ì´í•˜ëŠ” ë¹„ì¶”ì²œ)


```

### í›ˆë ¨ ì „ëµ: Curriculum Learning

**Phase 1: Random Agent ë¶€íŠ¸ìŠ¤íŠ¸ë© (ì´ˆê¸° í•™ìŠµ)**

ëª©ì : ê¸°ë³¸ì ì¸ í¬ì»¤ ê°œë… í•™ìŠµ ë° ë¬´ì‘ìœ„ í”Œë ˆì´ ê·¹ë³µ

```python
# ìƒëŒ€: Random Agent (ë¬´ì‘ìœ„ ì•¡ì…˜)
policies = {
    "learning_agent": Policy(MLP),      # í•™ìŠµ ì¤‘ì¸ ì—ì´ì „íŠ¸
    "random_opponent": RandomPolicy()   # ê³ ì •ëœ ëœë¤ ì—ì´ì „íŠ¸
}

policies_to_train = ["learning_agent"]  # learning_agentë§Œ í•™ìŠµ
```

**ì¢…ë£Œ ì¡°ê±´**:
- vs Random Agent ìŠ¹ë¥  **85%+** ë‹¬ì„±
- ì˜ˆìƒ ì‹œê°„: 1-2ì‹œê°„ (í•™ìŠµ í™˜ê²½ì— ë”°ë¼)

**í•™ìŠµ ë‚´ìš©**:
- ê¸°ë³¸ ë² íŒ… ê°œë… (í´ë“œ vs ì½œ)
- ëª…ë°±íˆ ë‚˜ìœ ì•¡ì…˜ íšŒí”¼
- ê³µê²©ì  í”Œë ˆì´ì˜ ì´ì  ì¸ì‹

---

**Phase 2: Self-Play vs Historical Checkpoints (ê³ ê¸‰ í•™ìŠµ)**

ëª©ì : ê³¼ê±° ìì‹ ê³¼ ëŒ€ê²°í•˜ë©° ì „ëµ ë°œì „ (League Training)

```python
# ìƒëŒ€: ê³¼ê±° ì²´í¬í¬ì¸íŠ¸ (ì£¼ê¸°ì  ì—…ë°ì´íŠ¸)
policies = {
    "learning_agent": Policy(MLP),           # í˜„ì¬ í•™ìŠµ ì¤‘
    "historical_opponent": Policy(MLP)       # ê³¼ê±° ì²´í¬í¬ì¸íŠ¸
}

policies_to_train = ["learning_agent"]

# ì²´í¬í¬ì¸íŠ¸ ì—…ë°ì´íŠ¸ ì£¼ê¸°
# ë§¤ 100 iterationsë§ˆë‹¤ historical_opponent ì •ì±… ì—…ë°ì´íŠ¸
```

**ì—…ë°ì´íŠ¸ ì „ëµ**:
```python
if iteration % 100 == 0:
    # í˜„ì¬ learning_agentë¥¼ historical_opponentë¡œ ë³µì‚¬
    save_checkpoint("learning_agent", f"checkpoint_{iteration}")
    load_checkpoint("historical_opponent", f"checkpoint_{iteration}")
```


**ì¢…ë£Œ ì¡°ê±´** (ëª¨ë‘ ì¶©ì¡± ì‹œ Phase 3ë¡œ ì „í™˜):
1. **vs Random Agent**: 95%+ ìŠ¹ë¥  (100 ê²Œì„ ê¸°ì¤€)
2. **vs Call Station**: 80%+ ìŠ¹ë¥  (100 ê²Œì„ ê¸°ì¤€)
   - Call Station: ê±°ì˜ ëª¨ë“  ìƒí™©ì—ì„œ ì½œë§Œ í•˜ëŠ” í”Œë ˆì´ì–´ (Fold 5%, Call 85%, Raise 10%)
   - Value betting ëŠ¥ë ¥ ê²€ì¦
3. **vs Historical Checkpoints**: ìµœê·¼ 10ê°œ ì²´í¬í¬ì¸íŠ¸ ëŒ€ë¹„ í‰ê·  55%+ ìŠ¹ë¥ 
4. **í•™ìŠµ ì•ˆì •ì„±**: ìµœê·¼ 100 iterations í‰ê·  ë³´ìƒì˜ í‘œì¤€í¸ì°¨ < 0.1
5. **ìµœì†Œ í•™ìŠµ ì‹œê°„**: 10ì‹œê°„ ì´ìƒ
6. **ì •ì±… ì—”íŠ¸ë¡œí”¼**: > 1.0 (ì•¡ì…˜ ë‹¤ì–‘ì„± ìœ ì§€)

**í•™ìŠµ ë‚´ìš©**:
- ë³µì¡í•œ ë² íŒ… íŒ¨í„´
- ë¸”ëŸ¬í•‘ê³¼ ë°¸ë¥˜ ë² íŒ… ê· í˜•
- ìƒëŒ€ ì „ëµ ì ì‘

---

**Phase 3: Self-Play (ìµœì¢… ë‹¨ê³„, ì„ íƒ)**

ëª©ì : ë‘ ì—ì´ì „íŠ¸ ë™ì‹œ í•™ìŠµìœ¼ë¡œ GTO ê·¼ì‚¬

```python
# ì–‘ìª½ ëª¨ë‘ í•™ìŠµ
policies = {
    "player_0": Policy(MLP),
    "player_1": Policy(MLP)
}

policies_to_train = ["player_0", "player_1"]
```

**íŠ¹ì§•**:
- ëŒ€ì¹­ì  í™˜ê²½ (ê³µì •ì„±)
- í”Œë ˆì´ì–´ ê°„ ìƒí˜¸ ë°œì „
- GTO ì „ëµì— ì ì§„ì  ìˆ˜ë ´

---

### RLlib êµ¬í˜„ ì˜ˆì‹œ

```python
# train.py
from ray.rllib.algorithms.ppo import PPOConfig

# Phase 1: vs Random
config_phase1 = (
    PPOConfig()
    .multi_agent(
        policies={
            "learning_agent": (None, obs_space, act_space, {}),
            "random_opponent": (None, obs_space, act_space, {"explore": True}),
        },
        policy_mapping_fn=lambda agent_id: 
            "learning_agent" if agent_id == "player_0" else "random_opponent",
        policies_to_train=["learning_agent"]
    )
)

# Phase 2: vs Historical
config_phase2 = (
    PPOConfig()
    .multi_agent(
        policies={
            "learning_agent": (None, obs_space, act_space, {}),
            "historical_opponent": (None, obs_space, act_space, {}),
        },
        policy_mapping_fn=lambda agent_id:
            "learning_agent" if agent_id == "player_0" else "historical_opponent",
        policies_to_train=["learning_agent"]
    )
    .callbacks(HistoricalCheckpointCallback)  # ì£¼ê¸°ì  ì—…ë°ì´íŠ¸
)
```

---


## ğŸ“Š í‰ê°€ ë°©ë²•

### í•™ìŠµ ì¤‘ ë©”íŠ¸ë¦­

**TensorBoard ë©”íŠ¸ë¦­**:
- `episode_reward_mean` - í‰ê·  ì—í”¼ì†Œë“œ ë³´ìƒ
- `episode_len_mean` - í‰ê·  í•¸ë“œ ìˆ˜
- `policy_loss` - ì •ì±… ì†ì‹¤
- `vf_loss` - ê°€ì¹˜ í•¨ìˆ˜ ì†ì‹¤
- `entropy` - ì •ì±… ì—”íŠ¸ë¡œí”¼

**ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­**:
- **bb/100** (í•„ìˆ˜!) - 100 í•¸ë“œë‹¹ íšë“ BB
- í‰ê·  íŒŸ í¬ê¸°
- VPIP (ìë°œì  íŒŸ ì°¸ì—¬ìœ¨)
- PFR (í”„ë¦¬í”Œë ë ˆì´ì¦ˆìœ¨)
- Aggression Factor
- í‰ê·  í•¸ë“œ ê¸¸ì´ (ì•¡ì…˜ ìˆ˜)

### ë²¤ì¹˜ë§ˆí¬ ì—ì´ì „íŠ¸

**1. Random Agent** - ë¬´ì‘ìœ„ ì•¡ì…˜
- ëª¨ë“  legal action ì¤‘ ê· ë“± í™•ë¥ ë¡œ ì„ íƒ
- ê°€ì¥ ì•½í•œ ë² ì´ìŠ¤ë¼ì¸

**2. Call Station** - ìˆ˜ë™ì  ì½œ ì¤‘ì‹¬ í”Œë ˆì´
- **ì •ì˜**: ê±°ì˜ ëª¨ë“  ìƒí™©ì—ì„œ ì½œë§Œ í•˜ëŠ” í”Œë ˆì´ì–´
- **í–‰ë™ ë¶„í¬**: Fold 5%, Call 85%, Raise 10%
- **íŠ¹ì§•**: 
  - ì•½í•œ í•¸ë“œë¡œë„ ëê¹Œì§€ ë”°ë¼ê°
  - ë¸”ëŸ¬í•‘ì— ê°•í•¨ (í´ë“œ ì•ˆ í•¨)
  - Value bettingì— ì·¨ì•½
- **ëª©ì **: AIì˜ value betting ëŠ¥ë ¥ ê²€ì¦

**3. Nit** - ë§¤ìš° íƒ€ì´íŠ¸í•œ í”Œë ˆì´ (Phase 3 ë²¤ì¹˜ë§ˆí¬)
- ì¢‹ì€ í•¸ë“œë§Œ í”Œë ˆì´
- ê³µê²©ì ì´ì§€ë§Œ ì˜ˆì¸¡ ê°€ëŠ¥
- í–¥í›„ êµ¬í˜„ ì˜ˆì •

**4. Historical Checkpoints** - ê³¼ê±° ìì‹ ê³¼ ëŒ€ê²°
- League trainingì˜ í•µì‹¬
- ê³¼ì í•© ë°©ì§€
- ì§€ì†ì ì¸ ìê¸° ê°œì„  ê²€ì¦

---

**í‰ê°€ ì§€í‘œ: bb/100 (í¬ì»¤ í‘œì¤€)**

**âš ï¸ ì¤‘ìš”**: ìŠ¹ë¥ ì€ ì˜ë¯¸ ì—†ëŠ” ì§€í‘œ!

```python
# âŒ ìŠ¹ë¥  (ì“¸ëª¨ì—†ìŒ!)
90 í•¸ë“œ ìŠ¹ë¦¬ (+10 BB)
10 í•¸ë“œ íŒ¨ë°° (-200 BB)
ìŠ¹ë¥ : 90% (ì¢‹ì•„ ë³´ì„)
ì‹¤ì œ: -190 BB (ë§í•¨!)

# âœ… bb/100 (í¬ì»¤ í‘œì¤€)
bb/100 = (ì´ íšë“ BB / í•¸ë“œ ìˆ˜) Ã— 100

ì˜ˆì‹œ:
1000 í•¸ë“œ, +500 BB
â†’ bb/100 = (500/1000) Ã— 100 = 50 bb/100
```

**ë²¤ì¹˜ë§ˆí¬ ëª©í‘œ (bb/100)**:

| Phase | ìƒëŒ€ | ëª©í‘œ bb/100 | ì˜ë¯¸ |
|-------|------|-------------|------|
| **Phase 1 ì¢…ë£Œ** | vs Random | **+80 bb/100** | Random ì••ë„ |
| **Phase 2 ì¢…ë£Œ** | vs Random | **+100 bb/100** | Random ì™„ë²½ ì§€ë°° |
| | vs Call Station | **+50 bb/100** | Value betting ëŠ¥ë ¥ |
| **Phase 3 ëª©í‘œ** | vs Nit | **+20 bb/100** | íƒ€ì´íŠ¸ í”Œë ˆì´ì–´ ëŒ€ì‘ |
| | vs Historical | **+10 bb/100** | ìê¸° ìì‹  ë„˜ì–´ì„œê¸° |

**bb/100 ê¸°ì¤€**:
- **+50 ì´ìƒ**: ë§¤ìš° ê°•í•¨
- **+20~50**: ê°•í•¨
- **+5~20**: ê´œì°®ìŒ
- **0~5**: ì•½ê°„ ì´ê¹€
- **0 ë¯¸ë§Œ**: ì§

**ì¸¡ì • ë°©ë²•**:
```python
def evaluate_bb100(agent, opponent, num_hands=1000):
    total_bb = 0
    
    for _ in range(num_hands):
        obs, info = env.reset()
        # í•¸ë“œ í”Œë ˆì´
        ...
        total_bb += final_bb_change
    
    bb_100 = (total_bb / num_hands) * 100
    return bb_100

# ì˜ˆì‹œ
bb100_vs_random = evaluate_bb100(agent, RandomAgent(), 1000)
print(f"vs Random: {bb100_vs_random:.1f} bb/100")
# ëª©í‘œ: +80 ì´ìƒ
```

---

## ğŸ—“ï¸ êµ¬í˜„ ë‹¨ê³„

### Phase 0: í™˜ê²½ êµ¬ì¶• (1-2ì¼)

- [ ] Ray/RLlib ì„¤ì¹˜ ë° í™˜ê²½ ì„¤ì •
- [ ] ì˜ì¡´ì„± ì •ë¦¬ (`requirements.txt`)
- [ ] í”„ë¡œì íŠ¸ êµ¬ì¡° ì„¤ê³„

### Phase 1: Gymnasium í™˜ê²½ êµ¬í˜„ (2-3ì¼)

- [ ] `PokerEnv` í´ë˜ìŠ¤ ìƒì„±
  - [ ] `__init__()` - ì´ˆê¸°í™”
  - [ ] `reset()` - ì—í”¼ì†Œë“œ ì‹œì‘
  - [ ] `step()` - ì•¡ì…˜ ì‹¤í–‰
  - [ ] `_get_observation()` - ê´€ì°° ìƒì„±
  - [ ] `_get_reward()` - ë³´ìƒ ê³„ì‚°
  - [ ] `_map_action()` - ì•¡ì…˜ ë§¤í•‘
- [ ] POKERENGINE í†µí•©
- [ ] í™˜ê²½ ê²€ì¦
  - [ ] `gymnasium.utils.env_checker` í†µê³¼
  - [ ] ìˆ˜ë™ í”Œë ˆì´ í…ŒìŠ¤íŠ¸

### Phase 2: RLlib í†µí•© (1-2ì¼)

- [ ] í™˜ê²½ ë“±ë¡
- [ ] PPO ì„¤ì •
- [ ] Multi-Agent ì„¤ì •
- [ ] í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„± (`train.py`)
- [ ] ì²« í•™ìŠµ ì‹¤í–‰ (5-10ë¶„)
- [ ] TensorBoard í™•ì¸

### Phase 3: ê¸°ë³¸ í•™ìŠµ (3-5ì¼)

- [ ] MLP ëª¨ë¸ë¡œ í•™ìŠµ
- [ ] í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
- [ ] í•™ìŠµ ì•ˆì •ì„± í™•ì¸
- [ ] ì²´í¬í¬ì¸íŠ¸ ì €ì¥/ë¡œë“œ
- [ ] í•™ìŠµ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ

### Phase 4: AI ëŒ€ì „ ì‹œìŠ¤í…œ (2ì¼)

- [ ] `play.py` - AI vs AI ì‹œë®¬ë ˆì´ì…˜
- [ ] `play_human.py` - ì‚¬ëŒ vs AI
- [ ] ê²Œì„ ë¡œê¹…
- [ ] í†µê³„ ìˆ˜ì§‘

### Phase 5: í‰ê°€ & ê°œì„  (ì§„í–‰ì¤‘)

- [ ] ë²¤ì¹˜ë§ˆí¬ ì—ì´ì „íŠ¸ êµ¬í˜„
- [ ] ì„±ëŠ¥ í‰ê°€
- [ ] ì „ëµ ë¶„ì„
- [ ] ë¬¸ì œì  ì‹ë³„ ë° í•´ê²°

### Phase 6: ê³ ê¸‰ ê¸°ëŠ¥ (ì„ íƒ)

- [ ] Transformer ëª¨ë¸ ì „í™˜
- [ ] ICM ë³´ìƒ ì‹¤í—˜
- [ ] ë¸”ë¼ì¸ë“œ ë ˆë²¨ì—… (í† ë„ˆë¨¼íŠ¸)
- [ ] ë©€í‹°í”Œë ˆì´ì–´ (3-9ì¸)
- [ ] ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ
- [ ] Opponent Modeling

---

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
glacial-supernova/
â”œâ”€â”€ POKERENGINE/              # ì»¤ìŠ¤í…€ í¬ì»¤ ì—”ì§„
â”‚   â”œâ”€â”€ poker_engine/
â”‚   â”œâ”€â”€ admin.py
â”‚   â””â”€â”€ test_poker_engine.py
â”œâ”€â”€ poker_rl/                 # ìƒˆ AI í•™ìŠµ í”„ë¡œì íŠ¸
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ env.py                # Gymnasium í™˜ê²½
â”‚   â”œâ”€â”€ config.py             # ì„¤ì • ë° í•˜ì´í¼íŒŒë¼ë¯¸í„°
â”‚   â”œâ”€â”€ train.py              # í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ play.py               # AI ëŒ€ì „
â”‚   â”œâ”€â”€ play_human.py         # ì‚¬ëŒ vs AI
â”‚   â”œâ”€â”€ models/               # ëª¨ë¸ ì•„í‚¤í…ì²˜
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ mlp.py
â”‚   â”‚   â””â”€â”€ transformer.py
â”‚   â”œâ”€â”€ agents/               # ë²¤ì¹˜ë§ˆí¬ ì—ì´ì „íŠ¸
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ random_agent.py
â”‚   â”‚   â””â”€â”€ call_station.py
â”‚   â””â”€â”€ utils/                # ìœ í‹¸ë¦¬í‹°
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ metrics.py
â”‚       â””â”€â”€ visualization.py
â”œâ”€â”€ experiments/              # ì‹¤í—˜ ê²°ê³¼
â”‚   â”œâ”€â”€ checkpoints/
â”‚   â””â”€â”€ logs/
â”œâ”€â”€ requirements.txt          # ì˜ì¡´ì„±
â”œâ”€â”€ README.md                 # í”„ë¡œì íŠ¸ ì„¤ëª…
â””â”€â”€ IMPLEMENTATION_PLAN.md    # ì´ ë¬¸ì„œ
```

---

## ğŸ”¬ ì‹¤í—˜ ê³„íš

### Experiment 1: Baseline

**ëª©í‘œ**: MLP + PPOë¡œ ê¸°ë³¸ í•™ìŠµ ê°€ëŠ¥ì„± í™•ì¸

**ì„¤ì •**:
- ëª¨ë¸: MLP (256-256-128)
- í•™ìŠµ ì‹œê°„: 2-4ì‹œê°„
- ëª©í‘œ: vs Random 80%+ ìŠ¹ë¥ 

### Experiment 2: Hyperparameter Tuning

**ëª©í‘œ**: ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰

**ë³€ìˆ˜**:
- Learning rate: [1e-4, 3e-4, 1e-3]
- Batch size: [2048, 4096, 8192]
- Entropy coeff: [0.01, 0.02, 0.05]

### Experiment 3: Model Comparison

**ëª©í‘œ**: MLP vs Transformer ì„±ëŠ¥ ë¹„êµ

**ì„¤ì •**:
- ë™ì¼í•œ í•™ìŠµ ì‹œê°„
- ë™ì¼í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°
- ìŠ¹ë¥  ë° í•™ìŠµ ì†ë„ ë¹„êµ

---

## âš ï¸ ì˜ˆìƒ ë¬¸ì œ ë° í•´ê²°ì±…

### ë¬¸ì œ 1: í•™ìŠµ ë¶ˆì•ˆì •

**ì¦ìƒ**: ë³´ìƒì´ ë°œì‚°í•˜ê±°ë‚˜ ìˆ˜ë ´í•˜ì§€ ì•ŠìŒ

**í•´ê²°ì±…**:
- Learning rate ê°ì†Œ
- Batch size ì¦ê°€
- Reward clipping
- ì •ê·œí™” ê°•í™”

### ë¬¸ì œ 2: ê³¼ì í•©

**ì¦ìƒ**: í•™ìŠµ ì—ì´ì „íŠ¸ë¼ë¦¬ë§Œ ì´ê¸°ê³  ìƒˆë¡œìš´ ìƒëŒ€ëŠ” ëª» ì´ê¹€

**í•´ê²°ì±…**:
- Population-based training
- League training
- ì •ê¸°ì ì¸ ì •ì±… ë¦¬ì…‹

### ë¬¸ì œ 3: íƒí—˜ ë¶€ì¡±

**ì¦ìƒ**: íŠ¹ì • ì „ëµì—ë§Œ ìˆ˜ë ´ (ì˜ˆ: í•­ìƒ í´ë“œ)

**í•´ê²°ì±…**:
- Entropy coefficient ì¦ê°€
- Curiosity-driven exploration
- ë³´ìƒ í•¨ìˆ˜ íŠœë‹

---

## ğŸ“š ì°¸ê³  ìë£Œ

### ë…¼ë¬¸
- "Deep Reinforcement Learning from Self-Play in Imperfect-Information Games" (DeepStack)
- "Superhuman AI for multiplayer poker" (Pluribus)
- "Mastering the Game of No-Limit Texas Hold'em Poker through Self-Play" (Slumbot)

### ì½”ë“œ
- RLlib ê³µì‹ ë¬¸ì„œ: https://docs.ray.io/en/latest/rllib/
- Gymnasium ë¬¸ì„œ: https://gymnasium.farama.org/
- POKERENGINE: ìš°ë¦¬ ì»¤ìŠ¤í…€ ì—”ì§„

### ë„êµ¬
- TensorBoard: í•™ìŠµ ëª¨ë‹ˆí„°ë§
- Ray Dashboard: ë¶„ì‚° í•™ìŠµ ëª¨ë‹ˆí„°ë§

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸ & ë§ˆì¼ìŠ¤í†¤

### Milestone 1: Environment Ready (1ì£¼)
- [x] POKERENGINE ì™„ì„± ë° í…ŒìŠ¤íŠ¸
- [ ] Gymnasium í™˜ê²½ êµ¬í˜„
- [ ] í™˜ê²½ ê²€ì¦ ì™„ë£Œ

### Milestone 2: First Training (2ì£¼)
- [ ] RLlib í†µí•©
- [ ] ì²« í•™ìŠµ ì‹¤í–‰ ì„±ê³µ
- [ ] TensorBoard í™•ì¸
- [ ] vs Random 50%+ ìŠ¹ë¥ 

### Milestone 3: Baseline Agent (1ê°œì›”)
- [ ] vs Random 90%+ ìŠ¹ë¥ 
- [ ] ê¸°ë³¸ ì „ëµ í•™ìŠµ í™•ì¸
- [ ] ì²´í¬í¬ì¸íŠ¸ ì €ì¥/ë¡œë“œ

### Milestone 4: Competitive Agent (2ê°œì›”)
- [ ] vs Call Station 80%+ ìŠ¹ë¥ 
- [ ] ë³µì¡í•œ ì „ëµ í•™ìŠµ
- [ ] ì‚¬ëŒê³¼ ëŒ€ê²° ì‹œìŠ¤í…œ

### Milestone 5: Advanced Features (3ê°œì›”+)
- [ ] Transformer ëª¨ë¸
- [ ] ë©€í‹°í”Œë ˆì´ì–´
- [ ] GTO ê·¼ì‚¬ ê²€ì¦

---

## ğŸ¯ ì„±ê³µ ê¸°ì¤€

**Phase 1 ì„±ê³µ**:
- AIê°€ ë¬´ì‘ìœ„ í”Œë ˆì´ì–´ë¥¼ 90% ì´ìƒ ì´ê¹€
- ê¸°ë³¸ì ì¸ í¬ì»¤ ê°œë… ì´í•´ (í´ë“œ, ë²³, ë ˆì´ì¦ˆ)

**Phase 2 ì„±ê³µ**:
- AIê°€ Call Stationì„ 80% ì´ìƒ ì´ê¹€
- ë²¨ë¥˜ ë² íŒ…ê³¼ ë¸”ëŸ¬í•‘ êµ¬ì‚¬

**ìµœì¢… ì„±ê³µ**:
- AIê°€ ìˆ™ë ¨ëœ ì•„ë§ˆì¶”ì–´ í”Œë ˆì´ì–´ì™€ ëŒ€ë“±í•˜ê²Œ ê²½ìŸ
- GTO ì „ëµì— ê·¼ì ‘í•˜ëŠ” í”Œë ˆì´
- ë‹¤ì–‘í•œ ìƒí™©ì—ì„œ ì ì‘ì  ì „ëµ

---

**ì‘ì„±ì¼**: 2025-11-30
**ì‘ì„±ì**: AI Assistant
**ë²„ì „**: 1.0
